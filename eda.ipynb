{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and Test variants shape :  (3321, 4) (5668, 3)\n",
      "Train and Test text shape :  (3321, 2) (5668, 2)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "train_variants_df = pd.read_csv(\"data/training_variants\")\n",
    "test_variants_df = pd.read_csv(\"data/test_variants\")\n",
    "train_text_df = pd.read_csv(\"data/training_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "test_text_df = pd.read_csv(\"data/test_text\", sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
    "\n",
    "print(\"Train and Test variants shape : \",train_variants_df.shape, test_variants_df.shape)\n",
    "print(\"Train and Test text shape : \",train_text_df.shape, test_text_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ACSL4</td>\n",
       "      <td>R570S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NAGLU</td>\n",
       "      <td>P521L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>PAH</td>\n",
       "      <td>L333F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ING1</td>\n",
       "      <td>A148D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>TMEM216</td>\n",
       "      <td>G77A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID     Gene Variation\n",
       "0   0    ACSL4     R570S\n",
       "1   1    NAGLU     P521L\n",
       "2   2      PAH     L333F\n",
       "3   3     ING1     A148D\n",
       "4   4  TMEM216      G77A"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_variants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Gene</th>\n",
       "      <th>Variation</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FAM58A</td>\n",
       "      <td>Truncating Mutations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>CBL</td>\n",
       "      <td>W802*</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>CBL</td>\n",
       "      <td>Q249E</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>CBL</td>\n",
       "      <td>N454D</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>CBL</td>\n",
       "      <td>L399V</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID    Gene             Variation  Class\n",
       "0   0  FAM58A  Truncating Mutations      1\n",
       "1   1     CBL                 W802*      2\n",
       "2   2     CBL                 Q249E      2\n",
       "3   3     CBL                 N454D      3\n",
       "4   4     CBL                 L399V      4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_variants_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Cyclin-dependent kinases (CDKs) regulate a var...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Abstract Background  Non-small cell lung canc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Recent evidence has demonstrated that acquired...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Oncogenic mutations in the monomeric Casitas B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Text\n",
       "0   0  Cyclin-dependent kinases (CDKs) regulate a var...\n",
       "1   1   Abstract Background  Non-small cell lung canc...\n",
       "2   2   Abstract Background  Non-small cell lung canc...\n",
       "3   3  Recent evidence has demonstrated that acquired...\n",
       "4   4  Oncogenic mutations in the monomeric Casitas B..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join variants data with text data\n",
    "training = train_variants_df.merge(right=train_text_df, on='ID')\n",
    "testing = test_variants_df.merge(right=test_text_df, on='ID')\n",
    "\n",
    "# join variants df together for comparison\n",
    "# though the test set won't be used to train the classifier\n",
    "train_variants_df['set'] = 'training'\n",
    "test_variants_df['set'] = 'testing'\n",
    "variants_df = pd.concat([train_variants_df, test_variants_df])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice there are 5 rows with null texts. We'll drop these rows.\n",
    "training.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at class distribution, only available in training set\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(x=\"Class\", data=train_variants_df, color='grey')\n",
    "# training.Class.value_counts().plot(kind='bar')\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlabel('Class number', fontsize=12)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title(\"Frequency of Classes\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an uneven distribution of each classes. What do each of these classes mean? A quick web search produced no answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 20 genes from the overall dataset\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(data=variants_df,\n",
    "             x='Gene',\n",
    "             hue='set',\n",
    "             order=variants_df.Gene.value_counts().iloc[:20].index)\n",
    "plt.xticks(rotation=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 20 Variations from the overall dataset\n",
    "# get top 20 variations to plot\n",
    "largest20_idx = variants_df.Variation.value_counts().nlargest(20).index\n",
    "largest20 = variants_df.loc[variants_df.Variation.isin(list(largest20_idx))]\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.countplot(data=largest20,\n",
    "             x='Variation',\n",
    "             hue='set',\n",
    "             )\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the top 20 Variations from the overall dataset by proportion\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.histplot(data = largest20,\n",
    "             x = 'Variation',\n",
    "             hue='set',\n",
    "             multiple=\"dodge\", \n",
    "             stat = 'density', #this will make the y-axis the density rather than count\n",
    "             shrink = 0.8, \n",
    "             common_norm=False, # this will normalize each density independently\n",
    "             )\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proportions of the variations in train/test sets seem similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(training[training['Gene'] == 'BRCA1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 9 most frequently mentioned genes vs their class counts\n",
    "fig, axs = plt.subplots(3,3, sharex=True, figsize=(12,5))\n",
    "plt.subplots_adjust(hspace=0.25)\n",
    "plt.suptitle(\"Gene vs Class Counts\", fontsize=18, y=1)\n",
    "for gene, ax in zip(list(training['Gene'].value_counts().nlargest(9).index),axs.ravel()):\n",
    "    training[training['Gene'] == gene].Class.value_counts().plot(kind='bar', ax=ax)\n",
    "    ax.set_title(f\"{gene}\", fontsize=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "\n",
    "# add common x and y labels    \n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.xlabel('Class number', fontsize=12)\n",
    "plt.grid(visible=False)\n",
    "plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single gene, there may be different mutations resulting in different classes\n",
    "\n",
    "Within a single paper, there may be descriptions of multiple variations which need multiple class labels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the paper call out specific mutations?\n",
    "mutations = ['E1735K','A1843T','D1778G']\n",
    "for mutation in mutations:\n",
    "    print(mutation in training.iloc[2417]['Text'])\n",
    "    \n",
    "# not necessarily?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training[\"num_words\"] = training[\"Text\"].apply(lambda x: len(str(x).split()) )\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.distplot(training.num_words.values, bins=50, kde=False, color='red')\n",
    "plt.xlabel('Number of words in text', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.title(\"Frequency of number of words\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents we are classifying are of various lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a specific document\n",
    "import textwrap\n",
    "print(textwrap.fill(train_text_df.loc[0,'Text'],88))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class vs keywords - NLTK\n",
    "Can we find keywords definining each class?\n",
    "\n",
    "plan:\n",
    "* break each document down into tokens\n",
    "* look at word frequency per class\n",
    "* remove stop words\n",
    "* remove numbers?\n",
    "* combine similar words (i.e., mutation vs mutations) - byte-pair encoding, Unigrams. We use the Huggingface Tokenizers library for these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/kfchou/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/kfchou/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this tokenizer keeps punctuations\n",
    "tokens = word_tokenize(train_text_df.loc[0,'Text'])\n",
    "fdist = FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words only\n",
    "tokenizer = RegexpTokenizer(pattern=r\"\\w+\") # uses regex to match delimiters\n",
    "tokens = tokenizer.tokenize(train_text_df.loc[0,'Text'])\n",
    "fdist = FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = pd.DataFrame([list(fdist.keys()), list(fdist.values())]).T\n",
    "token_dict.columns = ['token','count']\n",
    "token_dict = token_dict.sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords; i.e., common english words like 'a', 'an', 'the'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "token_dict = token_dict[~token_dict['token'].isin(stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=token_dict.head(30),\n",
    "    x='count',\n",
    "    y='token'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now do for classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(pattern=r\"\\w+\") \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokenize(str):\n",
    "    tokens = tokenizer.tokenize(str)\n",
    "    \n",
    "    # get word distribution\n",
    "    fdist = FreqDist(tokens) \n",
    "    \n",
    "    # return dataframe\n",
    "    token_dict = pd.DataFrame([list(fdist.keys()), list(fdist.values())]).T\n",
    "    token_dict.columns = ['token','count']\n",
    "    token_dict = token_dict.sort_values(by='count', ascending=False)\n",
    "    \n",
    "    # remove stopwords; i.e., common english words like 'a', 'an', 'the'\n",
    "    token_dict = token_dict[~token_dict['token'].isin(stop_words)]\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=token_dict.head(30),\n",
    "        x='count',\n",
    "        y='token'\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class1 = training[training['Class'] == 1]['Text'].str.cat(sep=' ')\n",
    "tokenize(class1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class vs keywords - HuggingFace\n",
    "https://towardsdatascience.com/training-bpe-wordpiece-and-unigram-tokenizers-from-scratch-using-hugging-face-3dd174850713"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the tokenizer and subword BPE trainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import (BpeTrainer, \n",
    "                                 WordLevelTrainer,\n",
    "                                 WordPieceTrainer,\n",
    "                                 UnigramTrainer)\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = \"<UNK>\"  # token for unknown words\n",
    "spl_tokens = [\"<UNK>\", \"<SEP>\", \"<MASK>\", \"<CLS>\"]  # special tokens\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "    \n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer\n",
    "\n",
    "def train_tokenizer(iterable, alg='WLV', use_pretrained=True):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        iterable - a python iterable\n",
    "        alg: WLV - word level algo\n",
    "             WPC - word piece algo\n",
    "             BPE - byte pair encoding\n",
    "             uni - unigram\n",
    "    \"\"\"\n",
    "    if use_pretrained:\n",
    "        try:\n",
    "            tokenizer = Tokenizer.from_file(f\"./{alg}_tokenizer-trained.json\")\n",
    "        except:\n",
    "            raise Exception(f'{alg}_tokenizer-trained.json is not found')\n",
    "    else:\n",
    "        tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "        tokenizer.train_from_iterator(iterable, trainer) # training the tokenzier\n",
    "        tokenizer.save(f\"./{alg}_tokenizer-trained.json\")\n",
    "        tokenizer = Tokenizer.from_file(f\"./{alg}_tokenizer-trained.json\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train tokenizers with four algorithms\n",
    "tokens_dict = {}\n",
    "trained_tokenizer = {}\n",
    "for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
    "    trained_tokenizer[alg] = train_tokenizer(training['Text'], alg)\n",
    "    # input_string = training[training['Class'] == 1]['Text'].str.cat(sep=' ')\n",
    "    # output = trained_tokenizer[alg].encode(input_string)\n",
    "    # tokens_dict[alg] = output.tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how each tokenizer breaks the input into tokens\n",
    "max_len = max(len(tokens_dict['UNI']), len(tokens_dict['WPC']), len(tokens_dict['BPE']))\n",
    "diff_bpe = max_len - len(tokens_dict['BPE'])\n",
    "diff_wpc = max_len - len(tokens_dict['WPC'])\n",
    "diff_mlv = max_len - len(tokens_dict['WLV'])\n",
    "tokens_dict['BPE'] = tokens_dict['BPE'] + ['<PAD>']*diff_bpe\n",
    "tokens_dict['WPC'] = tokens_dict['WPC'] + ['<PAD>']*diff_wpc\n",
    "tokens_dict['WLV'] = tokens_dict['WLV'] + ['<PAD>']*diff_mlv\n",
    "df = pd.DataFrame(tokens_dict)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_series = df.groupby('UNI')['UNI'].count().sort_values(ascending=False)\n",
    "token_dict = pd.DataFrame(token_series).rename(columns={'UNI':'count'}).rename_axis('token').reset_index()\n",
    "\n",
    "# remove stopwords; i.e., common english words like 'a', 'an', 'the'\n",
    "token_dict = token_dict[~token_dict['token'].isin(stop_words)]\n",
    "token_dict['len'] = token_dict['token'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    data=token_dict[token_dict['len']>2].head(30),\n",
    "    x='count',\n",
    "    y='token',\n",
    "    color='grey'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-specific tokens vs global tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords; i.e., common english words like 'a', 'an', 'the'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def tokens_distribution(encoded):\n",
    "    \"\"\"\n",
    "    given a hugging-face tokenized corpus, return a dataframe with columns:\n",
    "    * token\n",
    "    * count\n",
    "    * len - length characters in token\n",
    "    * percent - token count/total token count\n",
    "    \n",
    "    \"\"\"\n",
    "    # place tokens into dataframe & count occurance\n",
    "    tokens = pd.DataFrame(encoded.tokens)\n",
    "    tokens.columns = ['token']\n",
    "    tokens_df = tokens.groupby('token').value_counts().to_frame()\n",
    "    tokens_df.columns = ['count']\n",
    "    token_count_total = tokens_df['count'].sum()\n",
    "    \n",
    "    # remove stopwords; i.e., common english words like 'a', 'an', 'the'\n",
    "    tokens_df = tokens_df.reset_index()\n",
    "    tokens_df = tokens_df[~tokens_df['token'].isin(stop_words)]\n",
    "    tokens_df['len'] = tokens_df['token'].str.len()\n",
    "    \n",
    "    # remove short chars\n",
    "    tokens_df = tokens_df[tokens_df['len']>2]\n",
    "    \n",
    "    # proportions\n",
    "    tokens_df['percent'] = tokens_df['count']/token_count_total\n",
    "    return tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trained_tokenizer['WLV'].encode(training['Text'].str.cat(sep=' '))\n",
    "training_class_all_tokens_df = tokens_distribution(encoded=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token distributions for each training class\n",
    "training_class_tokens_df_dict = dict()\n",
    "for cls in range(9):\n",
    "    output = trained_tokenizer['WLV'].encode(training[training['Class']==cls+1]['Text'].str.cat(sep=' '))\n",
    "    training_class_tokens_df_dict[cls+1] = tokens_distribution(encoded=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "def plot_token_distribution_comparison(all_data_df, class_data_df_dict, cls1, cls2, topn=100):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        all_data_df: df with 'token' and 'count' columns\n",
    "        class_data_df_dict: dictionary containing class-specific dataframes\n",
    "        cls1, cls2: cancer mutation classes to compare\n",
    "        topn: number of most frequently occurring tokens to display\n",
    "    \n",
    "    \"\"\"\n",
    "    # merge class specific token distribution df with all token distribution df\n",
    "    tokens_df = all_data_df.merge(\n",
    "        right=class_data_df_dict[cls1], \n",
    "        on='token', \n",
    "        how='outer',\n",
    "        suffixes=[\"\",f\"_{cls1}\"]\n",
    "        ).merge(\n",
    "        right=class_data_df_dict[cls2], \n",
    "        on='token', \n",
    "        how='outer',\n",
    "        suffixes=[\"\",f\"_{cls2}\"]\n",
    "        )\n",
    "    tokens_df.head()\n",
    "    \n",
    "    # show top 100 most frequent tokens\n",
    "    topn_tokens = tokens_df.sort_values(by='count_all',ascending=False).head(topn).index\n",
    "\n",
    "    fig = px.scatter(\n",
    "        tokens_df.iloc[topn_tokens],\n",
    "        x='percent',\n",
    "        y=f'percent_{cls1}',\n",
    "        text='token',\n",
    "        log_x=True,\n",
    "        log_y=True\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        px.line(x=[0,0.05], y=[0,0.05]).data[0]\n",
    "    )\n",
    "    fig.update_traces(line_color='grey')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_token_distribution_comparison(training_class_all_tokens_df, training_class_tokens_df_dict, cls1=2, cls2=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls1 = 2\n",
    "cls2 = 7\n",
    "tokens_df = training_class_all_tokens_df.merge(\n",
    "        right=training_class_tokens_df_dict[cls1], \n",
    "        on='token', \n",
    "        how='outer',\n",
    "        suffixes=[\"\",f\"_{cls1}\"]\n",
    "        ).merge(\n",
    "        right=training_class_tokens_df_dict[cls2], \n",
    "        on='token', \n",
    "        how='outer',\n",
    "        suffixes=[\"\",f\"_{cls2}\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "topn_tokens = tokens_df.sort_values(by='count',ascending=False).head(100).index\n",
    "\n",
    "sns.scatterplot(x='percent_2', y='percent_7', hue=abs(tokens_df['percent_7'] - tokens_df['percent']), data=tokens_df.iloc[topn_tokens], alpha=0.1, size=2.5, sizes=(10, 250), edgecolor=None)\n",
    "sns.regplot(x='percent_2', y='percent_7', data=tokens_df.iloc[topn_tokens], scatter=False, color='gray')\n",
    "\n",
    "for index, row in tokens_df.iloc[topn_tokens].iterrows():\n",
    "    plt.text(row['percent_2'], row['percent_7'], row['token'], ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.xlabel(None)\n",
    "plt.ylabel('Class 7')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.suptitle('Frequency vs. Class 7')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn_tokens = tokens_df.sort_values(by='count',ascending=False).head(100).index\n",
    "\n",
    "sns.scatterplot(\n",
    "  data = tokens_df.iloc[topn_tokens],\n",
    "  x = 'percent_2',\n",
    "  y = 'percent_7',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create modified texts for training/testing, incorporating gene and variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = 'Gene: ' + training['Gene'] + '; Variation: ' + training['Variation'] + '; Report: ' + training['Text']\n",
    "Y_train = training['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PersonalizedMedicine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
